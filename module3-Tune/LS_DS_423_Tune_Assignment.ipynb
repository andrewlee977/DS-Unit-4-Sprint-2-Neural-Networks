{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "\n",
    "\n",
    "# Tune Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
    "\n",
    "# Gridsearch Hyperparameters\n",
    "\n",
    "In the guided project, you learned how to use sklearn's GridsearchCV and keras-tuner library to tune the hyperparameters of a neural network model. For your module project you'll continue using these two libraries however we are going to make things a little more interesting for you. \n",
    "\n",
    "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
    "\n",
    "\n",
    "\n",
    "**Don't forgot to switch to GPU on Colab!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# native python libraries imports \n",
    "import math\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# sklearn imports \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# keras imports \n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import get_file\n",
    "\n",
    "# required for compatibility between sklearn and keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quickdraw10():\n",
    "    \"\"\"\n",
    "    Fill out this doc string, and comment the code, for practice in writing the kind of code that will get you hired. \n",
    "    \"\"\"\n",
    "    \n",
    "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
    "    \n",
    "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
    "\n",
    "    data = np.load(path_to_zip)\n",
    "    \n",
    "    # normalize your image data\n",
    "    max_pixel_value = 255\n",
    "    X = data['arr_0']/max_pixel_value\n",
    "    Y = data['arr_1']\n",
    "        \n",
    "    return train_test_split(X, Y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_quickdraw10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Experiment 1\n",
    "\n",
    "## Tune Hyperperameters using Enhanced GridsearchCV \n",
    "\n",
    "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch. Specifically, we are going to automate away the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
    "\n",
    "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
    "\n",
    "\n",
    "### Objective \n",
    "\n",
    "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. Up until now, we've been manually selecting the number of layers and layer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
    "    \"\"\"\"\n",
    "    Returns a compiled keras model \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_layers: int \n",
    "        number of hidden layers in model \n",
    "        To be clear, this excludes the input and output layer.\n",
    "        \n",
    "    first_layer_nodes: int\n",
    "        Number of nodes in the first hidden layer \n",
    "\n",
    "    last_layer_nodes: int\n",
    "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
    "        \n",
    "     act_funct: string \n",
    "         Name of activation function to use in hidden layers (this excludes the output layler)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model: keras object \n",
    "    \"\"\"\n",
    "    \n",
    "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
    "        \"\"\"\n",
    "        Generates and returns the number of nodes in each hidden layer. \n",
    "        To be clear, this excludes the input and output layer. \n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        Number of nodes in each layer is linearly incremented. \n",
    "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers: int\n",
    "            Number of hidden layers\n",
    "            This values should be 2 or greater \n",
    "\n",
    "        first_layer_nodes: int\n",
    "\n",
    "        last_layer_nodes: int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        layers: list of ints\n",
    "            Contains number of nodes for each layer \n",
    "        \"\"\"\n",
    "\n",
    "        # throws an error if n_layers is less than 2 \n",
    "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
    "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
    "        # when set to True number of nodes are decreased for subsequent layers \n",
    "        if negative_node_incrementation:\n",
    "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
    "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
    "            \n",
    "        # when set to False number of nodes are increased for subsequent layers\n",
    "        else:\n",
    "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
    "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
    "\n",
    "        nodes = first_layer_nodes\n",
    "\n",
    "        for i in range(1, n_layers+1):\n",
    "\n",
    "            layers.append(math.ceil(nodes))\n",
    "\n",
    "            # increment nodes for next layer \n",
    "            nodes = nodes + nodes_increment\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
    "    \n",
    "    for i in range(1, n_layers):\n",
    "        if i==1:\n",
    "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
    "        else:\n",
    "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
    "            \n",
    "            \n",
    "    # output layer \n",
    "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
    "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='adam', # adam is a good default optimizer \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # do not include model.fit() inside the create_model function\n",
    "    # KerasClassifier is expecting a complied model \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore create_model\n",
    "\n",
    "Let's build a few different models in order to understand how the above code works in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 10` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dcf5c585f07629a03086cf57ba53615",
     "grade": false,
     "grade_id": "cell-86d63e89a21223de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "model = create_model(n_layers=10, \n",
    "                     first_layer_nodes=500, \n",
    "                     last_layer_nodes=100,\n",
    "                     act_funct=\"relu\",\n",
    "                     negative_node_incrementation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 456)               228456    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 412)               188284    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 367)               151571    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 323)               118864    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 278)               90072     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 234)               65286     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 189)               44415     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 145)               27550     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1460      \n",
      "=================================================================\n",
      "Total params: 1,308,458\n",
      "Trainable params: 1,308,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 10` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0722533c325d699f4842e874e43720e",
     "grade": false,
     "grade_id": "cell-99d563a291231a7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "model = create_model(n_layers=10,\n",
    "                     first_layer_nodes=500,\n",
    "                     last_layer_nodes=100,\n",
    "                     act_funct=\"relu\",\n",
    "                     negative_node_incrementation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 545)               273045    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 589)               321594    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 634)               374060    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 678)               430530    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 723)               490917    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 767)               555308    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 812)               623616    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 856)               695928    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                8570      \n",
      "=================================================================\n",
      "Total params: 4,166,068\n",
      "Trainable params: 4,166,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "# Notice in the model summary how the number of nodes have been linearly incremented in increasing values.\n",
    "# The output layer must have 10 nodes because there are 10 labels to predict \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to play around with parameters to gain additional insight as to how the create_model function works \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we've played around a bit with  `create_model` in order to understand how it works, let's build a much simpler model that we'll be running gridsearches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 2` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "606b85d0ba4531836f97caf6850297f8",
     "grade": false,
     "grade_id": "cell-4ca6c5e51302fd10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "model = create_model(n_layers=2,\n",
    "                     first_layer_nodes=500,\n",
    "                     last_layer_nodes=100,\n",
    "                     act_funct=\"relu\",\n",
    "                     negative_node_incrementation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 397,510\n",
      "Trainable params: 397,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {'n_layers': [2, 3],\n",
    "              'epochs': [3], \n",
    "              \"first_layer_nodes\": [500, 300],\n",
    "              \"last_layer_nodes\": [100, 50]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Epoch 1/3\n",
      "2344/2344 [==============================] - 5s 2ms/step - loss: 0.7419 - accuracy: 0.7696\n",
      "Epoch 2/3\n",
      "2344/2344 [==============================] - 5s 2ms/step - loss: 0.4046 - accuracy: 0.8777\n",
      "Epoch 3/3\n",
      "2344/2344 [==============================] - 5s 2ms/step - loss: 0.3266 - accuracy: 0.8993\n",
      "Best: 0.8662533362706503 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
      "Means: 0.8640133341153463, Stdev: 0.0004434365555616086 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.863320012887319, Stdev: 0.002761063167250236 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
      "Means: 0.8625600139300028, Stdev: 0.0006819681111869349 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
      "Means: 0.8662533362706503, Stdev: 0.0013331802775852358 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
      "Means: 0.8614000082015991, Stdev: 0.0021409122298674636 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.8631999890009562, Stdev: 0.0019669346196400715 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
      "Means: 0.8599733312924703, Stdev: 0.004252119082566397 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
      "Means: 0.862559994061788, Stdev: 0.0023868615852519603 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n"
     ]
    }
   ],
   "source": [
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-2, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 3,\n",
       " 'first_layer_nodes': 500,\n",
       " 'last_layer_nodes': 50,\n",
       " 'n_layers': 3,\n",
       " 'build_fn': <function __main__.create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct='relu', negative_node_incrementation=True)>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Experiment 2\n",
    "\n",
    "## Benchmark different Optimization Algorithms \n",
    "\n",
    "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
    "\n",
    "- Random Search\n",
    "- Bayesian Optimization. \n",
    "- Brute Force Gridsearch\n",
    "\n",
    "Our goal in this experiment is two-fold. We want to see which appraoch \n",
    "\n",
    "- Scores the highest accuracy\n",
    "- Has the shortest run time \n",
    "\n",
    "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "`Brute Force Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
    "\n",
    "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the highest possible accuracies. \n",
    "\n",
    "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which greatly influence the model learning outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
    "# let's build a simple model to minimize run time \n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units=hp.get('units'),\n",
    "                    activation=hp.get(\"activation\")))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hp = HyperParameters()\n",
    "hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
    "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Run the Gridsearch Algorithms \n",
    "\n",
    "### Random Search\n",
    "\n",
    "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaff9aae33845f374e15f2381719d83a",
     "grade": false,
     "grade_id": "cell-8c1dfb9b6d12bea2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# how many unique hyperparameter combinations do we have? \n",
    "# HINT: take the product of the number of possible values for each hyperparameter \n",
    "# save your answer to n_unique_hparam_combos\n",
    "\n",
    "n_unique_hparam_combos = 16 * 3 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9d628451e83431e1b52da10eccf2c00",
     "grade": false,
     "grade_id": "cell-1fa83950bb2d5f92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# how many of these do we want to randomly sample?\n",
    "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
    "# save this number to n_param_combos_to_sample\n",
    "\n",
    "n_param_combos_to_sample = n_unique_hparam_combos * .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./keras-tuner-trial/random_search/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from ./keras-tuner-trial/random_search/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "random_tuner = RandomSearch(\n",
    "            build_model,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
    "            seed=1234,\n",
    "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
    "            directory='./keras-tuner-trial',\n",
    "            project_name='random_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 09s]\n",
      "val_accuracy: 0.6783199906349182\n",
      "\n",
      "Best val_accuracy So Far: 0.8737599849700928\n",
      "Total elapsed time: 00h 00m 09s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# take note of Total elapsed time in print out\n",
    "random_tuner.search(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial/random_search\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8737599849700928\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8723199963569641\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 448\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8642399907112122\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 416\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8641600012779236\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 480\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8624799847602844\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8605200052261353\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 224\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8593199849128723\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 160\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8428400158882141\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 192\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8425999879837036\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8402799963951111\n"
     ]
    }
   ],
   "source": [
    "# identify the best score and hyperparameter (should be at the top since scores are ranked)\n",
    "random_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f084b5d373f8589a1de8d6d4473b974a",
     "grade": true,
     "grade_id": "cell-5527738b6382c164",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The best hyperparameter combination:\n",
    "\n",
    "units (number of nodes in hidden layer): 384\n",
    "learning_rate: 0.001\n",
    "activation: relu\n",
    "Score: **0.8737599849700928**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Bayesian Optimization\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
    "\n",
    "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
    "\n",
    "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
    "\n",
    "`num_initial_points`: \n",
    "\n",
    "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine liklihood of which param combo to try next based on expected improvement\n",
    "\n",
    "\n",
    "`beta`: \n",
    "\n",
    "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
    "\n",
    "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
    "# because BO isn't random (after num_initial_points number of trails) let's see if 15 max trials gives good results\n",
    "# feel free to play with any of these numbers\n",
    "max_trials=15\n",
    "num_initial_points=5\n",
    "beta=5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_tuner = BayesianOptimization(\n",
    "                    build_model,\n",
    "                    objective='val_accuracy',\n",
    "                    max_trials=max_trials,\n",
    "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
    "                    num_initial_points=num_initial_points, \n",
    "                    beta=beta, \n",
    "                    seed=1234,\n",
    "                    directory='./keras-tuner-trial',\n",
    "                    project_name='bayesian_optimization_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 00m 12s]\n",
      "val_accuracy: 0.829800009727478\n",
      "\n",
      "Best val_accuracy So Far: 0.8696799874305725\n",
      "Total elapsed time: 00h 02m 36s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner.search(X_train, y_train,\n",
    "               epochs=3,\n",
    "               validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8696799874305725\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8690000176429749\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 192\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8675600290298462\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 480\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.865559995174408\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8611599802970886\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8394399881362915\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.01\n",
      "activation: relu\n",
      "Score: 0.829800009727478\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8257200121879578\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.01\n",
      "activation: relu\n",
      "Score: 0.8245199918746948\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8117600083351135\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparameter combination and model score. \n",
    "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
     "grade": true,
     "grade_id": "cell-ff95600bf745f40f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Best performing hyperparameter combination:\n",
    "\n",
    "units (# of nodes in hidden layer): 512\n",
    "learning_rate: 0.001\n",
    "activation: relu\n",
    "Score: 0.8696799874305725"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Brute Force Gridsearch Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate a Sklearn compatiable parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hyper_parameters = {\n",
    "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
    "    \"units\": np.arange(32, 544, 32).tolist(),\n",
    "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
    "    \"activation\":[\"relu\", \"sigmoid\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units': [32,\n",
       "  64,\n",
       "  96,\n",
       "  128,\n",
       "  160,\n",
       "  192,\n",
       "  224,\n",
       "  256,\n",
       "  288,\n",
       "  320,\n",
       "  352,\n",
       "  384,\n",
       "  416,\n",
       "  448,\n",
       "  480,\n",
       "  512],\n",
       " 'learning_rate': [0.1, 0.01, 0.001],\n",
       " 'activation': ['relu', 'sigmoid']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Sklearn compatiable model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(units, learning_rate, activation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units, activation=activation))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn = build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "2344/2344 [==============================] - 6s 2ms/step - loss: 0.7620 - accuracy: 0.7681\n",
      "Best: 0.8443866570790609 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
      "Means: 0.26015999913215637, Stdev: 0.016146133454328273 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
      "Means: 0.23089333872000375, Stdev: 0.03950219151167123 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
      "Means: 0.23331999778747559, Stdev: 0.05749393285365964 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
      "Means: 0.2549733320871989, Stdev: 0.046159144226003236 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
      "Means: 0.27057332793871564, Stdev: 0.010955938544944786 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
      "Means: 0.3025600115458171, Stdev: 0.04060236073837775 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
      "Means: 0.26918667058149975, Stdev: 0.029520328507326547 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
      "Means: 0.2916533350944519, Stdev: 0.01593554996099375 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
      "Means: 0.28099999328454334, Stdev: 0.07430583788989635 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
      "Means: 0.26177332798639935, Stdev: 0.012160408244223537 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
      "Means: 0.3155866662661235, Stdev: 0.050184779428324 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
      "Means: 0.2643733322620392, Stdev: 0.057087682109497634 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
      "Means: 0.28650666773319244, Stdev: 0.052726289786407414 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
      "Means: 0.24002666274706522, Stdev: 0.03001031029700425 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
      "Means: 0.2654000024000804, Stdev: 0.02905698461347357 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
      "Means: 0.29321332772572833, Stdev: 0.038838291587059225 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 512}\n",
      "Means: 0.7792399923006693, Stdev: 0.008618913543436026 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
      "Means: 0.7978933254877726, Stdev: 0.007708100595540229 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
      "Means: 0.7970666686693827, Stdev: 0.002988747872973792 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
      "Means: 0.8023333152135214, Stdev: 0.010360560848433899 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
      "Means: 0.8032533526420593, Stdev: 0.00374112801807014 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
      "Means: 0.8068533341089884, Stdev: 0.006852517389743349 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
      "Means: 0.8012400070826212, Stdev: 0.008383107568441857 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
      "Means: 0.8033733367919922, Stdev: 0.005410732709990525 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
      "Means: 0.7970133423805237, Stdev: 0.003521552343577337 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
      "Means: 0.7981866598129272, Stdev: 0.002822057666321281 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
      "Means: 0.8044266502062479, Stdev: 0.004982769170062462 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
      "Means: 0.8058000008265177, Stdev: 0.003739678449749728 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
      "Means: 0.8077200055122375, Stdev: 0.004284802362890978 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
      "Means: 0.7996800144513448, Stdev: 0.004499610234836568 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
      "Means: 0.8041599988937378, Stdev: 0.001333865545397416 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
      "Means: 0.8008400003115336, Stdev: 0.001956586642321212 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 512}\n",
      "Means: 0.7844133377075195, Stdev: 0.005484758250088885 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
      "Means: 0.8038266698519388, Stdev: 0.004325109115667538 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
      "Means: 0.8224800030390421, Stdev: 0.00311143793362772 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
      "Means: 0.8230400085449219, Stdev: 0.002765137086278375 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
      "Means: 0.8265466690063477, Stdev: 0.0017289187707563943 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
      "Means: 0.8326266606648763, Stdev: 0.0012416295828206218 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
      "Means: 0.8341333270072937, Stdev: 0.004203410401710732 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
      "Means: 0.8349733153978983, Stdev: 0.0003597540842107445 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
      "Means: 0.8388933340708414, Stdev: 0.002398742627542727 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
      "Means: 0.8380399942398071, Stdev: 0.002174012241334476 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
      "Means: 0.8335333267847697, Stdev: 0.0015257307088161669 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
      "Means: 0.8336266676584879, Stdev: 0.004574985417339066 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
      "Means: 0.841533342997233, Stdev: 0.0026218137146817528 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
      "Means: 0.8387733101844788, Stdev: 0.002456736303005821 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
      "Means: 0.8417733311653137, Stdev: 0.00426587552470617 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
      "Means: 0.8443866570790609, Stdev: 0.0018363341624861728 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
      "Means: 0.6548666755358378, Stdev: 0.023997901864244692 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
      "Means: 0.638866662979126, Stdev: 0.019935933734242742 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
      "Means: 0.6492533286412557, Stdev: 0.01464329951141856 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
      "Means: 0.6633866628011068, Stdev: 0.022931006013165868 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
      "Means: 0.6354000171025594, Stdev: 0.026696604095049183 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
      "Means: 0.6327066620190939, Stdev: 0.02506677275296214 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
      "Means: 0.6094133257865906, Stdev: 0.036529687599662304 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
      "Means: 0.6233999927838644, Stdev: 0.012914665470874526 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
      "Means: 0.6292933424313863, Stdev: 0.02984469324400993 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
      "Means: 0.6519200007120768, Stdev: 0.015290504917171864 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
      "Means: 0.6490400036176046, Stdev: 0.03305388928881376 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
      "Means: 0.634600023428599, Stdev: 0.011310495358973143 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
      "Means: 0.6543200016021729, Stdev: 0.00979296584107536 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
      "Means: 0.6620133320490519, Stdev: 0.0007348936000329504 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
      "Means: 0.6140533288319906, Stdev: 0.03865257437906346 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
      "Means: 0.6397200028101603, Stdev: 0.015801277948340603 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 512}\n",
      "Means: 0.7903333306312561, Stdev: 0.0015144947611949152 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
      "Means: 0.8002533515294393, Stdev: 0.001141380959081097 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
      "Means: 0.8026533126831055, Stdev: 0.009133342944082381 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
      "Means: 0.8150533437728882, Stdev: 0.005234983792556654 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
      "Means: 0.8155199885368347, Stdev: 0.0044693918614185455 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
      "Means: 0.8207466801007589, Stdev: 0.002278325863971322 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
      "Means: 0.8063733379046122, Stdev: 0.012740378752074477 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
      "Means: 0.8172133366266886, Stdev: 0.004103359728173504 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
      "Means: 0.8125333388646444, Stdev: 0.0049247412939409 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
      "Means: 0.8192266623179117, Stdev: 0.003957296398948785 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
      "Means: 0.8195733428001404, Stdev: 0.0024423753379412424 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
      "Means: 0.8156399925549825, Stdev: 0.0064449247325808106 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
      "Means: 0.8129066824913025, Stdev: 0.00869544780885236 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
      "Means: 0.8189866542816162, Stdev: 0.009240501447518082 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
      "Means: 0.8086933493614197, Stdev: 0.007670291120891905 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
      "Means: 0.8145600159962972, Stdev: 0.0024194696582268897 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 512}\n",
      "Means: 0.7616133292516073, Stdev: 0.0019520392279971947 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
      "Means: 0.7772533098856608, Stdev: 0.0013632594626963289 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
      "Means: 0.7836399873097738, Stdev: 0.003588567972527993 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
      "Means: 0.7912533283233643, Stdev: 0.0005369900599430468 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
      "Means: 0.79366668065389, Stdev: 0.0022965099368091554 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
      "Means: 0.792520006497701, Stdev: 0.0045555601633332655 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
      "Means: 0.7974799871444702, Stdev: 0.001956594765862952 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
      "Means: 0.798253337542216, Stdev: 0.0010784385479848302 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
      "Means: 0.798906664053599, Stdev: 0.005656969657903072 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
      "Means: 0.8013066649436951, Stdev: 0.0023606370152313098 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
      "Means: 0.7981733282407125, Stdev: 0.0027937538140674133 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
      "Means: 0.7936133146286011, Stdev: 0.008400783543914844 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
      "Means: 0.8018933335940043, Stdev: 0.0006599754939857823 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
      "Means: 0.801360011100769, Stdev: 0.002489024285659358 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
      "Means: 0.7992799878120422, Stdev: 0.0029558518483077476 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n",
      "Means: 0.798853317896525, Stdev: 0.004741582266789323 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 512}\n"
     ]
    }
   ],
   "source": [
    "# save start time \n",
    "start = time()\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=hyper_parameters, \n",
    "                    n_jobs=-2, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# save end time \n",
    "end = time()\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.08546868165334"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total run time \n",
    "total_run_time_in_miniutes = (end - start)/60\n",
    "total_run_time_in_miniutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu', 'learning_rate': 0.001, 'units': 512}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 905us/step - loss: 0.4921 - accuracy: 0.8562\n"
     ]
    }
   ],
   "source": [
    "# because all other optimization approaches are reporting test set score\n",
    "# let's calculate the test set score in this case \n",
    "best_model = grid_result.best_estimator_\n",
    "test_acc = best_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8561999797821045"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9577db883482c6cded3836e5cfbf5a74",
     "grade": true,
     "grade_id": "cell-eb06d682d2790f6e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Best hyperparameter combination:\n",
    "\n",
    "Best Score: **0.8443866570790609** using\n",
    "activation: 'relu',\n",
    "learning_rate: 0.001,\n",
    "units: 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
    "\n",
    "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Stretch Goals\n",
    "\n",
    "- Feel free to run whatever gridserach experiments on whatever models you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is your open playground - be free to explore as you wish "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_433_Tune_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
